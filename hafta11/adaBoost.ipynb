{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides: https://cs229.stanford.edu/notes2021fall/lecture11-boosting.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "y_column = 'target'\n",
    "sample_weights_col = 'sample_weight'\n",
    "def shuffle_data(x,y):\n",
    "    data  = list(zip(x,y))\n",
    "    np.random.shuffle(data)\n",
    "    return list(zip(*data))\n",
    "\n",
    "# prepare pandas from sklearn classification data\n",
    "def sklearn_dataset_to_pandas(X, y)-> Tuple[pd.DataFrame,list[str]]:\n",
    "    x_columns = [f'feature{i}' for i in range(len(X[0]))]\n",
    "    df = pd.DataFrame(X, columns=x_columns)\n",
    "    df[y_column] = y\n",
    "    return df, x_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3, 1), (2, 3, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_data([1,2,3],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 0.8\n",
    "sample = 125\n",
    "train_size = int(sample * split)\n",
    "X, y = make_classification(n_samples=sample, n_features=4,\n",
    "                           n_informative=4, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "X,y = shuffle_data(X,y)\n",
    "Xtrain, Xtest = X[:train_size], X[train_size:]\n",
    "ytrain, ytest = y[:train_size], y[train_size:]\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=500, algorithm=\"SAMME\", random_state=0)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "clf.score(Xtest, ytest) # 93.5% mean accuracy on the whole datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4, 100, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain), len(Xtrain[0]), len(ytrain), ytrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,x_columns = sklearn_dataset_to_pandas(Xtrain,ytrain)\n",
    "train_df[sample_weights_col] = [1. for _ in range(len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'feature0' in train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feature0', 'feature1', 'feature2', 'feature3', 'target',\n",
       "       'sample_weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "from typing import  Tuple\n",
    "\n",
    "\n",
    "def node_entropy(node_df: pd.DataFrame) -> float:\n",
    "\n",
    "    # Initialize to uniform sample weights if it is not defined\n",
    "    if sample_weights_col not in node_df.columns:\n",
    "        node_df[sample_weights_col] = [1/len(node_df)] * len(node_df)\n",
    "    count = sum(node_df[sample_weights_col])\n",
    "\n",
    "    class_labels = node_df[y_column].unique()\n",
    "\n",
    "    impurity = 0\n",
    "\n",
    "    for c in class_labels:\n",
    "        pc = sum(node_df[node_df[y_column]==c][sample_weights_col])/count\n",
    "        if pc > 0 :\n",
    "            impurity -= pc * np.log2(pc)\n",
    "    \n",
    "    return impurity\n",
    "\n",
    "def get_class(df:pd.DataFrame) -> str:\n",
    "    counts = df[y_column].value_counts()\n",
    "    return counts.idxmax()\n",
    "\n",
    "def binary_split(node_df:pd.DataFrame,col_name:str) -> Tuple[float,float]:\n",
    "    all_values = list(set(node_df[col_name]))\n",
    "    all_values.sort()\n",
    "    min_impurity = float('inf')\n",
    "    best_split_val = None\n",
    "    n = len(node_df)\n",
    "    class_labels = []\n",
    "    for val in all_values[:-1]: # no need to check last element\n",
    "        left = node_df[node_df[col_name]<=val]\n",
    "        left_class = get_class(left)\n",
    "        right = node_df[node_df[col_name]>val]\n",
    "        right_class = get_class(right)\n",
    "        impurity = (node_entropy(left) * len(left) + node_entropy(right) * len(right)) / n\n",
    "        if impurity < min_impurity:\n",
    "            min_impurity = impurity\n",
    "            best_split_val = val\n",
    "            class_labels = [left_class, right_class]\n",
    "    return min_impurity, best_split_val, class_labels\n",
    "\n",
    "def find_best_split(node_df:pd.DataFrame) -> Tuple[float,str, float,list[int]]:\n",
    "    best_col = ''\n",
    "    best_impurity = float('inf')\n",
    "    best_split_val = None\n",
    "    best_class_labels = None\n",
    "    for col in x_columns:\n",
    "        impurity, split_val,class_labels = binary_split(node_df,col)\n",
    "        print(\"Best impurity for feature {} is {} at split value {}\".format(col, impurity, split_val))\n",
    "        if impurity < best_impurity:\n",
    "            best_impurity = impurity\n",
    "            best_col = col\n",
    "            best_split_val = split_val\n",
    "            best_class_labels = class_labels\n",
    "\n",
    "    return best_impurity, best_col, best_split_val, best_class_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records([\n",
    "    {y_column:1,sample_weights_col: 1},\n",
    "    {y_column:0,sample_weights_col: 1},\n",
    "])\n",
    "#change weights above to see how impacts the node entropy\n",
    "node_entropy(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best impurity for feature feature0 is 0.9606604868900653 at split value -2.0881450513286834\n",
      "Best impurity for feature feature1 is 0.8348796487340618 at split value -0.17958168172007427\n",
      "Best impurity for feature feature2 is 0.9216705383525391 at split value 0.43346834758261643\n",
      "Best impurity for feature feature3 is 0.969330601745662 at split value 2.7734453614461563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8348796487340618, 'feature1', -0.17958168172007427, [0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[10][sample_weights_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def weight_coefficient(weighted_error:float)->float:\n",
    "    weighted_error = min(weighted_error,0.9999)\n",
    "    return max(0.0001,np.log((1- weighted_error)/weighted_error) / 2)\n",
    "\n",
    "def update_sample_weights(train_df, weight_coefficient, predictions)-> pd.DataFrame:\n",
    "    sample_weights = list(train_df[sample_weights_col].copy())\n",
    "    for index, row in train_df.iterrows():\n",
    "        pred = predictions[index]\n",
    "        if pred != row[y_column]:\n",
    "            sample_weights[index] *= np.exp(weight_coefficient)\n",
    "        else:\n",
    "            sample_weights[index] /= np.exp(weight_coefficient)\n",
    "    return sample_weights\n",
    "\n",
    "class DecisionNode:\n",
    "    def __init__(self,data: pd.DataFrame, \n",
    "                      node_type: Literal['numerical']= 'numerical'):\n",
    "        self.node_type = node_type # only support numerical for now\n",
    "        self.impurity, self.col, self.decision_boundary, self.class_labels = find_best_split(data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Decision node with impurity {self.impurity} at column {self.col} with decision boundary {self.decision_boundary}\"\n",
    "    \n",
    "    \n",
    "    def predict_single(self,sample): \n",
    "        return self.class_labels[(sample[self.col] >= self.decision_boundary).astype(int)]\n",
    "    \n",
    "    def predict(self,data: pd.DataFrame)-> pd.Series:\n",
    "        return data.apply(self.predict_single,axis=1)\n",
    "    \n",
    "    def error(self,data: pd.DataFrame)-> float:\n",
    "        predictions = self.predict(data)\n",
    "        errors = (predictions != data[\"target\"]).astype(int)\n",
    "        return np.average(errors, weights=data[sample_weights_col])\n",
    "    \n",
    "class EnsembleModel:\n",
    "    def __init__(self,estimators: list[DecisionNode], estimator_weights:list[float]):\n",
    "        self.estimators = estimators\n",
    "        self.estimator_weights = estimator_weights\n",
    "\n",
    "    def predict(self,X):\n",
    "        predictions = np.array([est.predict(X).apply(lambda x: x if x==1. else -1.) for est in self.estimators])\n",
    "        return (np.average(predictions, weights=self.estimator_weights, axis=0) > 0 ).astype(int)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost algorithm\n",
    "\n",
    "\"\"\"\n",
    "1- Initialize uniform sample importance weights\n",
    "2- for t in range(T): \n",
    "    - Train classifier using sample weights. \n",
    "    - Compute weight coefficient for the classifier. \n",
    "    - Update sample weights based on classifier errors.\n",
    "    - Normalize the sample weights.\n",
    "3- Return final classifier as a weighted sum of the base classifiers.\n",
    "\"\"\"\n",
    "def adaboost(train_df,number_of_estimators:int):\n",
    "    estimators = []\n",
    "    estimator_weights = []\n",
    "    for t in range(number_of_estimators):\n",
    "        \n",
    "        estimator_t = DecisionNode(train_df)\n",
    "        error = estimator_t.error(train_df)\n",
    "        coeff = weight_coefficient(error)\n",
    "        print(f\"Iteration {t}, Estimator: {estimator_t}, weighted_error: {error}, coefficient: {coeff}\")\n",
    "\n",
    "        new_sample_weights = update_sample_weights(train_df,coeff,estimator_t.predict(train_df))\n",
    "        train_df[sample_weights_col] = new_sample_weights\n",
    "        estimators.append(estimator_t)\n",
    "        estimator_weights.append(coeff)\n",
    "    return EnsembleModel(estimators=estimators,estimator_weights=estimator_weights)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "sample = 250\n",
    "train_size = int(sample * split)\n",
    "X, y = make_classification(n_samples=sample, n_features=2,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           n_clusters_per_class=2,\n",
    "                           random_state=0, shuffle=False)\n",
    "X,y = shuffle_data(X,y)\n",
    "Xtrain, Xtest = X[:train_size], X[train_size:]\n",
    "ytrain, ytest = y[:train_size], y[train_size:]\n",
    "train_df,x_columns = sklearn_dataset_to_pandas(Xtrain,ytrain)\n",
    "train_df[sample_weights_col] = [1. for _ in range(len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best impurity for feature feature0 is 0.9163789976352388 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.4339601350573218 at split value 0.287669079338688\n",
      "Iteration 0, Estimator: Decision node with impurity 0.4339601350573218 at column feature1 with decision boundary 0.287669079338688, weighted_error: 0.095, coefficient: 1.1270290260496927\n",
      "Best impurity for feature feature0 is 0.9030266731499583 at split value 0.773128958812364\n",
      "Best impurity for feature feature1 is 0.7187420326252159 at split value -1.010732051113941\n",
      "Iteration 1, Estimator: Decision node with impurity 0.7187420326252159 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.33061936609479503, coefficient: 0.35269254776564596\n",
      "Best impurity for feature feature0 is 0.8593917747744035 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7436378115108586 at split value -1.010732051113941\n",
      "Iteration 2, Estimator: Decision node with impurity 0.7436378115108586 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8593573507099217 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7436261760088585 at split value -1.010732051113941\n",
      "Iteration 3, Estimator: Decision node with impurity 0.7436261760088585 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5000499999998335, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8593229190808833 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7436145300663247 at split value -1.010732051113941\n",
      "Iteration 4, Estimator: Decision node with impurity 0.7436145300663247 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5000999999986667, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8592884798880311 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7436028736839456 at split value -1.010732051113941\n",
      "Iteration 5, Estimator: Decision node with impurity 0.7436028736839456 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5001499999955002, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8592540331321068 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7435912068624101 at split value -1.010732051113941\n",
      "Iteration 6, Estimator: Decision node with impurity 0.7435912068624101 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5001999999893334, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8592195788138574 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.743579529602409 at split value -1.010732051113941\n",
      "Iteration 7, Estimator: Decision node with impurity 0.743579529602409 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5002499999791667, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8591851169340262 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7435678419046318 at split value -1.010732051113941\n",
      "Iteration 8, Estimator: Decision node with impurity 0.7435678419046318 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5002999999640002, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8591506474933567 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7435561437697683 at split value -1.010732051113941\n",
      "Iteration 9, Estimator: Decision node with impurity 0.7435561437697683 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5003499999428336, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8591161704926012 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7435444351985138 at split value -1.010732051113941\n",
      "Iteration 10, Estimator: Decision node with impurity 0.7435444351985138 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.500399999914667, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.859081685932498 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7435327161915566 at split value -1.010732051113941\n",
      "Iteration 11, Estimator: Decision node with impurity 0.7435327161915566 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5004499998785003, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8590471938137983 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7435209867495925 at split value -1.010732051113941\n",
      "Iteration 12, Estimator: Decision node with impurity 0.7435209867495925 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5004999998333337, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8590126941372556 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.743509246873317 at split value -1.010732051113941\n",
      "Iteration 13, Estimator: Decision node with impurity 0.743509246873317 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.500549999778167, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8589781869036046 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7434974965634181 at split value -1.010732051113941\n",
      "Iteration 14, Estimator: Decision node with impurity 0.7434974965634181 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5005999997120004, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8589436721136029 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7434857358205949 at split value -1.010732051113941\n",
      "Iteration 15, Estimator: Decision node with impurity 0.7434857358205949 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5006499996338338, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8589091497679955 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.743473964645541 at split value -1.010732051113941\n",
      "Iteration 16, Estimator: Decision node with impurity 0.743473964645541 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5006999995426674, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8588746198675294 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.743462183038951 at split value -1.010732051113941\n",
      "Iteration 17, Estimator: Decision node with impurity 0.743462183038951 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.500749999437501, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.8588400824129628 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7434503910015269 at split value -1.010732051113941\n",
      "Iteration 18, Estimator: Decision node with impurity 0.7434503910015269 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.5007999993173344, coefficient: 0.0001\n",
      "Best impurity for feature feature0 is 0.858805537405037 at split value -1.4564920975406506\n",
      "Best impurity for feature feature1 is 0.7434385885339597 at split value -1.010732051113941\n",
      "Iteration 19, Estimator: Decision node with impurity 0.7434385885339597 at column feature1 with decision boundary -1.010732051113941, weighted_error: 0.500849999181168, coefficient: 0.0001\n"
     ]
    }
   ],
   "source": [
    "ensemble_model = adaboost(train_df=train_df,number_of_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ensemble_model.predict(train_df) == np.array(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ensemble_model.estimators[1].predict(train_df) == np.array(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
