{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides: https://cs229.stanford.edu/notes2021fall/lecture11-boosting.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "y_column = 'target'\n",
    "sample_weights_col = 'sample_weight'\n",
    "def shuffle_data(x,y):\n",
    "    data  = list(zip(x,y))\n",
    "    np.random.shuffle(data)\n",
    "    return list(zip(*data))\n",
    "\n",
    "# prepare pandas from sklearn classification data\n",
    "def sklearn_dataset_to_pandas(X, y)-> Tuple[pd.DataFrame,list[str]]:\n",
    "    x_columns = [f'feature{i}' for i in range(len(X[0]))]\n",
    "    df = pd.DataFrame(X, columns=x_columns)\n",
    "    df[y_column] = y\n",
    "    return df, x_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data([1,2,3],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "sample = 125\n",
    "train_size = int(sample * split)\n",
    "X, y = make_classification(n_samples=sample, n_features=4,\n",
    "                           n_informative=4, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "X,y = shuffle_data(X,y)\n",
    "Xtrain, Xtest = X[:train_size], X[train_size:]\n",
    "ytrain, ytest = y[:train_size], y[train_size:]\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=500, algorithm=\"SAMME\", random_state=0)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "clf.score(Xtest, ytest) # 93.5% mean accuracy on the whole datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Xtrain), len(Xtrain[0]), len(ytrain), ytrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,x_columns = sklearn_dataset_to_pandas(Xtrain,ytrain)\n",
    "train_df[sample_weights_col] = [1. for _ in range(len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'feature0' in train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "from typing import  Tuple\n",
    "\n",
    "\n",
    "def node_entropy(node_df: pd.DataFrame) -> float:\n",
    "\n",
    "    # Initialize to uniform sample weights if it is not defined\n",
    "    if sample_weights_col not in node_df.columns:\n",
    "        node_df[sample_weights_col] = [1/len(node_df)] * len(node_df)\n",
    "    count = sum(node_df[sample_weights_col])\n",
    "\n",
    "    class_labels = node_df[y_column].unique()\n",
    "\n",
    "    impurity = 0\n",
    "\n",
    "    for c in class_labels:\n",
    "        pc = sum(node_df[node_df[y_column]==c][sample_weights_col])/count\n",
    "        if pc > 0 :\n",
    "            impurity -= pc * np.log2(pc)\n",
    "    \n",
    "    return impurity\n",
    "\n",
    "\n",
    "def binary_split(node_df:pd.DataFrame,col_name:str) -> Tuple[float,float]:\n",
    "    all_values = list(set(node_df[col_name]))\n",
    "    all_values.sort()\n",
    "    min_impurity = float('inf')\n",
    "    best_split_val = None\n",
    "    n = len(node_df)\n",
    "    for val in all_values:\n",
    "        left = node_df[node_df[col_name]<=val]\n",
    "        right = node_df[node_df[col_name]>val]\n",
    "        impurity = (node_entropy(left) * len(left) + node_entropy(right) * len(right)) / n\n",
    "        if impurity < min_impurity:\n",
    "            min_impurity = impurity\n",
    "            best_split_val = val\n",
    "    return min_impurity, best_split_val\n",
    "\n",
    "def find_best_split(node_df:pd.DataFrame) -> Tuple[float,str, float]:\n",
    "    best_col = ''\n",
    "    best_impurity = float('inf')\n",
    "    best_split_val = None\n",
    "    for col in x_columns:\n",
    "        impurity, split_val = binary_split(node_df,col)\n",
    "        print(\"Best impurity for feature {} is {} at split value {}\".format(col, impurity, split_val))\n",
    "        if impurity < best_impurity:\n",
    "            best_impurity = impurity\n",
    "            best_col = col\n",
    "            best_split_val = split_val\n",
    "\n",
    "    return best_impurity, best_col, best_split_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records([\n",
    "    {y_column:1,sample_weights_col: 2},\n",
    "    {y_column:0,sample_weights_col: 10},\n",
    "])\n",
    "#change weights above to see how impacts the node entropy\n",
    "node_entropy(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[10][sample_weights_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def weight_coefficient(weighted_error:float)->float:\n",
    "    weighted_error = min(weighted_error,0.9999)\n",
    "    return max(0.0001,np.log((1- weighted_error)/weighted_error) / 2)\n",
    "\n",
    "def update_sample_weights(train_df,weight_coefficient,predictions)-> pd.DataFrame:\n",
    "    sample_weights = list(train_df[sample_weights_col].copy())\n",
    "    for index, row in train_df.iterrows():\n",
    "        pred = predictions[index]\n",
    "        if pred != row[y_column]:\n",
    "            sample_weights[index] *= np.exp(weight_coefficient)\n",
    "        else:\n",
    "            sample_weights[index] /= np.exp(weight_coefficient)\n",
    "    return sample_weights\n",
    "\n",
    "class DecisionNode:\n",
    "    def __init__(self,data: pd.DataFrame, \n",
    "                      node_type: Literal['numerical']= 'numerical'):\n",
    "        self.node_type = node_type # only support numerical for now\n",
    "        self.impurity, self.col, self.decision_boundary = find_best_split(data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Decision node with impurity {self.impurity} at column {self.col} with decision boundary {self.decision_boundary}\"\n",
    "    \n",
    "    \n",
    "    def predict_single(self,sample): \n",
    "        return (sample[self.col] >= self.decision_boundary).astype(int)\n",
    "    \n",
    "    def predict(self,data: pd.DataFrame)-> pd.Series:\n",
    "        return data.apply(self.predict_single,axis=1)\n",
    "    \n",
    "    def error(self,data: pd.DataFrame)-> float:\n",
    "        predictions = self.predict(data)\n",
    "        errors = (predictions != data[\"target\"]).astype(int)\n",
    "        return np.average(errors, weights=data[sample_weights_col])\n",
    "    \n",
    "class EnsembleModel:\n",
    "    def __init__(self,estimators: list[DecisionNode], estimator_weights:list[float]):\n",
    "        self.estimators = estimators\n",
    "        self.estimator_weights = estimator_weights\n",
    "\n",
    "    def predict(self,X):\n",
    "        predictions = np.array([est.predict(X).apply(lambda x: x if x==1. else -1.) for est in self.estimators])\n",
    "        print(predictions)\n",
    "        return (np.average(predictions, weights=self.estimator_weights, axis=0) > 0 ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost algorithm\n",
    "\n",
    "\"\"\"\n",
    "1- Initialize uniform sample importance weights\n",
    "2- for t in range(T): \n",
    "    - Train classifier using sample weights. \n",
    "    - Compute weight coefficient for the classifier. \n",
    "    - Update sample weights based on classifier errors.\n",
    "    - Normalize the sample weights.\n",
    "3- Return final classifier as a weighted sum of the base classifiers.\n",
    "\"\"\"\n",
    "def adaboost(train_df,t:int):\n",
    "    estimators = []\n",
    "    estimator_weights = []\n",
    "    for t in range(T):\n",
    "        estimator_t = DecisionNode(train_df)\n",
    "        error = estimator_t.error(train_df)\n",
    "        coeff = weight_coefficient(error)\n",
    "        print(f\"Iteration {t}, Estimator: {estimator_t}, weighted_error: {error}, coefficient: {coeff}\")\n",
    "        new_sample_weights = update_sample_weights(train_df,coeff,estimator_t.predict(train_df))\n",
    "        train_df[sample_weights_col] = new_sample_weights\n",
    "        estimators.append(estimator_t)\n",
    "        estimator_weights.append(coeff)\n",
    "    return EnsembleModel(estimators=estimators,estimator_weights=estimator_weights)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "sample = 125\n",
    "train_size = int(sample * split)\n",
    "X, y = make_classification(n_samples=sample, n_features=3,\n",
    "                           n_informative=3, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "X,y = shuffle_data(X,y)\n",
    "Xtrain, Xtest = X[:train_size], X[train_size:]\n",
    "ytrain, ytest = y[:train_size], y[train_size:]\n",
    "train_df,x_columns = sklearn_dataset_to_pandas(Xtrain,ytrain)\n",
    "train_df[sample_weights_col] = [1. for _ in range(len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ensemble_model.predict(train_df) == np.array(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ensemble_model.estimators[0].predict(train_df) == np.array(ytrain))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
